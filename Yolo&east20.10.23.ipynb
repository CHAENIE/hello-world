{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "model='C:/deep/opencv/model/yolov3.weights'\n",
    "config='C:/deep/opencv/model/yolov3.cfg'\n",
    "class_labels='C:/deep/opencv/model/coco.names'\n",
    "confThreshold=0.5\n",
    "nmsThreshold=0.4\n",
    "\n",
    "img_files=['C:/deep/opencv/image/dog.jpg', 'C:/deep/opencv/image/person.jpg',\n",
    "           'C:/deep/opencv/image/yolo_01.jpg', 'C:/deep/opencv/image/sheep.jpg','C:/deep/opencv/image/kite.jpg']\n",
    "\n",
    "net=cv2.dnn.readNet(model, config)\n",
    "\n",
    "if net.empty():\n",
    "    print('Net open failed!')\n",
    "    sys.exit()\n",
    "    \n",
    "classes=[]\n",
    "with open(class_labels, 'rt') as f:\n",
    "    classes=f.read().rstrip('\\n').split('\\n')\n",
    "    \n",
    "colors=np.random.uniform(0, 255, size=(len(classes),3))\n",
    "\n",
    "layer_names=net.getLayerNames()\n",
    "output_layers=[layer_names[i[0]-1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "#실행\n",
    "for f in img_files:\n",
    "    img=cv2.imread(f)\n",
    "    if img is None:\n",
    "        continue\n",
    "        \n",
    "    #블롭 생선 & 추론\n",
    "    blob=cv2.dnn.blobFromImage(img, 1/255., (320,320), swapRB=True)\n",
    "    net.setInput(blob)\n",
    "    outs=net.forward(output_layers)\n",
    "    \n",
    "    h,w=img.shape[:2]\n",
    "    class_ids=[]\n",
    "    confidences=[]\n",
    "    boxes=[]\n",
    "    \n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores=detection[5:]\n",
    "            class_id=np.argmax(scores)\n",
    "            confidence=scores[class_id]\n",
    "            if confidence > confThreshold:\n",
    "                #바운딩 박스 중심 좌표 & 박스 크기\n",
    "                cx=int(detection[0]*w)\n",
    "                cy=int(detection[1]*h)\n",
    "                bw=int(detection[2]*w)\n",
    "                bh=int(detection[3]*h)\n",
    "                #바운딩 박스 좌상단 좌표\n",
    "                sx=int(cx-bw/2)\n",
    "                sy=int(cy-bh/2)\n",
    "                boxes.append([sx, sy, bw, bh])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(int(class_id))\n",
    "                \n",
    "    #비최대 억제\n",
    "    indices=cv2.dnn.NMSBoxes(boxes, confidences, confThreshold, nmsThreshold)\n",
    "    for i in indices:\n",
    "        i=i[0]\n",
    "        sx, sy, bw, bh=boxes[i]\n",
    "        label=f'{classes[class_ids[i]]}: {confidences[i]:.2}'\n",
    "        color=colors[class_ids[i]]\n",
    "        cv2.rectangle(img, (sx, sy, bw, bh), color, 2)\n",
    "        cv2.putText(img, label, (sx,sy-10),\n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,0,255), 1, cv2.LINE_AA)\n",
    "    \n",
    "    t, _=net.getPerfProfile()\n",
    "    label='Inference time: %.2f ms' % (t *1000.0/cv2.getTickFrequency())\n",
    "    cv2.putText(img, label, (10,30), cv2.FONT_HERSHEY_SIMPLEX, 0.7,\n",
    "               (0,0,255),1, cv2.LINE_AA)\n",
    "\n",
    "    cv2.imshow('img', img)\n",
    "    cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EAST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import math\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def decode(scores, geometry, scoreThreshold):\n",
    "    detections=[]\n",
    "    confidences=[]\n",
    "    height=scores.shape[2]\n",
    "    width=scores.shape[3]\n",
    "    \n",
    "    for y in range(0, height):\n",
    "        #Extract data from scores\n",
    "        scoresData=scores[0][0][y]\n",
    "        x0_data=geometry[0][0][y]\n",
    "        x1_data=geometry[0][1][y]\n",
    "        x2_data=geometry[0][2][y]\n",
    "        x3_data=geometry[0][3][y]\n",
    "        anglesData=geometry[0][4][y]\n",
    "        \n",
    "        for x in range(0, width):\n",
    "            score=scoresData[x]\n",
    "            if(score < scoreThreshold):\n",
    "                continue\n",
    "            #feature map은 320x320 블롭의 1/4크기이므로, 다시 4배 확대\n",
    "            offsetX=x*4.0\n",
    "            offsetY=y*4.0\n",
    "            angle=anglesData[x]\n",
    "            #(offsetX, offsetY) 위치에서 회전된 사각형 정보 추출\n",
    "            cosA=math.cos(angle)\n",
    "            sinA=math.sin(angle)\n",
    "            h=x0_data[x]+x2_data[x]\n",
    "            w=x1_data[x]+x3_data[x]\n",
    "            #회전된 사각형의 한쪽 모서리 점 좌표 계산\n",
    "            offset=([offsetX+cosA*x1_data[x]+sinA*x2_data[x],\n",
    "                    offsetY-sinA*x1_data[x]+cosA*x2_data[x]])\n",
    "            #회전된 사각형의 대각선에 위치한 두 모서리 점 좌표 계산\n",
    "            p1=(-sinA*h+offset[0], -cosA*h+offset[1])\n",
    "            p3=(-cosA*w+offset[0], sinA*w+offset[1])\n",
    "            center=((p1[0]+p3[0])/2, (p1[1]+p3[1])/2)\n",
    "            \n",
    "            detections.append((center, (w,h), -1*angle*180.0/math.pi))\n",
    "            confidences.append(float(score))\n",
    "    return [detections, confidences]\n",
    "\n",
    "#모델&설정 파일\n",
    "model='C:/deep/opencv/model/frozen_east_text_detection.pb'\n",
    "confThreshold=0.5\n",
    "nmsThreshold=0.4\n",
    "\n",
    "#테스트 이미지 파일\n",
    "img_files=['C:/deep/opencv/image/road_closed.jpg','C:/deep/opencv/image/patient.jpg','C:/deep/opencv/image/ocr_ex2.jpg']\n",
    "#네트워크 생성\n",
    "net=cv2.dnn.readNet(model)\n",
    "if net.empty():\n",
    "    print('Net open failed!')\n",
    "    sys.exit()\n",
    "#출력 레이어 이름 받아오기\n",
    "'''\n",
    "layer_names=net.getLayerNames()\n",
    "output_layers=[layer_names[i[0]-1] for i in net.getUnconnectedOutLayers()]\n",
    "print(output_layers)\n",
    "'''\n",
    "#실행\n",
    "for f in img_files:\n",
    "    img=cv2.imread(f)\n",
    "    if img is None:\n",
    "        continue\n",
    "        \n",
    "    #블롭 생선 & 추론\n",
    "    blob=cv2.dnn.blobFromImage(img, 1, (320,320), (123.68, 116.78, 103.94),True)\n",
    "    net.setInput(blob)\n",
    "    scores, geometry=net.forward(['feature_fusion/Conv_7/Sigmoid', 'feature_fusion/concat_3'])\n",
    "    #scores.shape=(1,1,80,80)\n",
    "    #geonetry.shape=(1,5,80,80)\n",
    "    \n",
    "    #score가 confThreshold보다 큰 RBOX 정보를 RotatedRect 형식으로 변환하여 반환\n",
    "    [boxes, confidences]=decode(scores, geometry, confThreshold)\n",
    "    #회전된 사각형에 대한 비최대 억제\n",
    "    indices=cv2.dnn.NMSBoxesRotated(boxes, confidences, confThreshold, nmsThreshold)\n",
    "    rw=img.shape[1]/320\n",
    "    rh=img.shape[0]/320\n",
    "    \n",
    "    for i in indices:\n",
    "        #회전된 사각형의 네 모서리 점 좌표 계산 & 표시\n",
    "        vertices=cv2.boxPoints(boxes[i[0]])\n",
    "        \n",
    "        for j in range(4):\n",
    "            vertices[j][0] *= rw\n",
    "            vertices[j][1] *= rh\n",
    "            \n",
    "        for j in range(4):\n",
    "            p1=(vertices[j][0], vertices[j][1])\n",
    "            p2=(vertices[(j+1)%4][0], vertices[(j+1)%4][1])\n",
    "            cv2.line(img, p1, p2, (0,0,255), 2, cv2.LINE_AA)\n",
    "            \n",
    "    cv2.imshow('img',img)\n",
    "    cv2.waitKey()\n",
    "    \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 가격표 EAST & OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#원하는 이미지 잘라내기\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pytesseract\n",
    "import math\n",
    "\n",
    "#모델&설정 파일\n",
    "model='C:/deep/opencv/model/frozen_east_text_detection.pb'\n",
    "confThreshold=0.5\n",
    "nmsThreshold=0.4\n",
    "\n",
    "#가격표 좌표로 전처리\n",
    "src=cv2.imread('C:/deep/opencv/image/ocr_ex2.jpg')\n",
    "\n",
    "if src is None:\n",
    "    print('image load failed')\n",
    "    sys.exit()\n",
    "\n",
    "w,h=600,480\n",
    "srcQuad=np.array([[353, 494],[762, 486],[772, 738],[351, 747]], np.float32)\n",
    "dstQuad=np.array([[0,0],[w-1,0],[w-1,h-1],[0,h-1]], np.float32)\n",
    "\n",
    "pers=cv2.getPerspectiveTransform(srcQuad, dstQuad)\n",
    "dst=cv2.warpPerspective(src,pers,(w,h))\n",
    "\n",
    "#테스트 이미지 파일\n",
    "img_files=['C:/deep/opencv/image/road_closed.jpg','C:/deep/opencv/image/patient.jpg','C:/deep/opencv/image/ocr_ex2.jpg']\n",
    "#네트워크 생성\n",
    "net=cv2.dnn.readNet(model)\n",
    "if net.empty():\n",
    "    print('Net open failed!')\n",
    "    sys.exit()\n",
    "#출력 레이어 이름 받아오기\n",
    "'''\n",
    "layer_names=net.getLayerNames()\n",
    "output_layers=[layer_names[i[0]-1] for i in net.getUnconnectedOutLayers()]\n",
    "print(output_layers)\n",
    "'''\n",
    "#실행\n",
    "for f in img_files:\n",
    "    img=cv2.imread(f)\n",
    "    if img is None:\n",
    "        continue\n",
    "        \n",
    "    #블롭 생선 & 추론\n",
    "    blob=cv2.dnn.blobFromImage(img, 1, (320,320), (123.68, 116.78, 103.94),True)\n",
    "    net.setInput(blob)\n",
    "    scores, geometry=net.forward(['feature_fusion/Conv_7/Sigmoid', 'feature_fusion/concat_3'])\n",
    "    #scores.shape=(1,1,80,80)\n",
    "    #geonetry.shape=(1,5,80,80)\n",
    "    \n",
    "    #score가 confThreshold보다 큰 RBOX 정보를 RotatedRect 형식으로 변환하여 반환\n",
    "    [boxes, confidences]=decode(scores, geometry, confThreshold)\n",
    "    #회전된 사각형에 대한 비최대 억제\n",
    "    indices=cv2.dnn.NMSBoxesRotated(boxes, confidences, confThreshold, nmsThreshold)\n",
    "    rw=img.shape[1]/320\n",
    "    rh=img.shape[0]/320\n",
    "    \n",
    "    for i in indices:\n",
    "        #회전된 사각형의 네 모서리 점 좌표 계산 & 표시\n",
    "        vertices=cv2.boxPoints(boxes[i[0]])\n",
    "        \n",
    "        for j in range(4):\n",
    "            vertices[j][0] *= rw\n",
    "            vertices[j][1] *= rh\n",
    "            \n",
    "        for j in range(4):\n",
    "            p1=(vertices[j][0], vertices[j][1])\n",
    "            p2=(vertices[(j+1)%4][0], vertices[(j+1)%4][1])\n",
    "            cv2.line(img, p1, p2, (0,0,255), 2, cv2.LINE_AA)\n",
    "            \n",
    "    cv2.imshow('img',img)\n",
    "    cv2.waitKey()\n",
    "    \n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "#tesseract-ocr\n",
    "print(pytesseract.image_to_string(dst, lang='Hangul+eng'))\n",
    "img_gray=cv2.cvtColor(dst, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "cv2.imshow('src', src)\n",
    "cv2.imshow('dst',dst)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVENT_LBUTTONDOWN: 146, 237\n",
      "EVENT_LBUTTONDOWN: 547, 224\n",
      "EVENT_LBUTTONDOWN: 514, 437\n",
      "EVENT_LBUTTONDOWN: 191, 460\n"
     ]
    }
   ],
   "source": [
    "#객체 좌표 찾기\n",
    "import sys\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "oldx=oldy=-1\n",
    "\n",
    "def on_mouse(event, x,y, flags, param):\n",
    "    global oldx, oldy\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        oldx, oldy=x,y\n",
    "        print('EVENT_LBUTTONDOWN: %d, %d' % (x,y))\n",
    "\n",
    "src=cv2.imread('C:/deep/opencv/image/ocr_ex12.jpg')\n",
    "\n",
    "cv2.namedWindow('image')\n",
    "cv2.setMouseCallback('image', on_mouse, src)\n",
    "\n",
    "\n",
    "cv2.imshow('image', src)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\f",
      "\n"
     ]
    }
   ],
   "source": [
    "#원하는 이미지 잘라내기\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pytesseract\n",
    "\n",
    "src=cv2.imread('C:/deep/opencv/image/ocr_ex12.jpg')\n",
    "\n",
    "if src is None:\n",
    "    print('image load failed')\n",
    "    sys.exit()\n",
    "\n",
    "w,h=600,480\n",
    "srcQuad=np.array([[146, 237],[547, 224],[514, 437],[191, 460]], np.float32)\n",
    "dstQuad=np.array([[0,0],[w-1,0],[w-1,h-1],[0,h-1]], np.float32)\n",
    "\n",
    "pers=cv2.getPerspectiveTransform(srcQuad, dstQuad)\n",
    "dst=cv2.warpPerspective(src,pers,(w,h))\n",
    "img_gray=cv2.cvtColor(dst, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "#tesseract-ocr\n",
    "print(pytesseract.image_to_string(img_gray, lang='Hangul+eng'))\n",
    "\n",
    "\n",
    "cv2.imshow('src', src)\n",
    "cv2.imshow('dst',img_gray)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pdf참조"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\f",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pytesseract\n",
    "\n",
    "#좌측상단부터 반시계방향으로 0,1,2,3번 좌표\n",
    "def reorderPts(pts):\n",
    "    idx=np.lexsort((pts[:,1], pts[:, 0 ]))# 칼럼 0 --> 칼럼 1 순으로 정렬한 인덱스를 반환\n",
    "    pts=pts[idx] # x 좌표로 정렬(idx순서대로 정렬)\n",
    "    if pts[0, 1]> pts[1, 1]:\n",
    "        pts[[0,1]]=pts[[1,0]]\n",
    "\n",
    "    if pts[2, 1]< pts[3, 1]:\n",
    "        pts[[2,3]]=pts[[3,2]]\n",
    "    return pts\n",
    "\n",
    "#filename='ocr_ex4.jpg'\n",
    "\n",
    "#if len(sys.argv)> 1:\n",
    "#    filename=sys.argv[1]\n",
    "\n",
    "src=cv2.imread('C:/deep/opencv/image/unnamed4.jpg')\n",
    "\n",
    "dw, dh= 720, 400\n",
    "srcQuad=np.array([[0,0],[0,0],[0,0],[0,0]],np.float32)\n",
    "dstQuad=np.array([[0,0],[0,dh],[dw,dh],[dw,0]], np.float32)#좌측상단부터 반시계방향\n",
    "dst=np.zeros((dh,dw), np.uint8)\n",
    "\n",
    "src_gray=cv2.cvtColor(src, cv2.COLOR_BGR2GRAY)\n",
    "th, src_bin=cv2.threshold(src_gray, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n",
    "#외곽선 검출\n",
    "contours, _ =cv2.findContours(src_bin, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "\n",
    "for pts in contours:\n",
    "    #너무 작은 객체 제외\n",
    "    if cv2.contourArea(pts) < 1000:\n",
    "        continue\n",
    "    #외곽선 근사화\n",
    "    approx=cv2.approxPolyDP(pts, cv2.arcLength(pts, True)* 0.02, True)\n",
    "    #컨벡스가 아니면 제외\n",
    "    if not cv2.isContourConvex(approx) or len(approx) != 4:\n",
    "        continue\n",
    "    srcQuad=reorderPts(approx.reshape(4, 2).astype(np.float32))\n",
    "    pers=cv2.getPerspectiveTransform(srcQuad, dstQuad)\n",
    "    dst=cv2.warpPerspective(src, pers, (dw, dh), flags=cv2.INTER_CUBIC)\n",
    "    dst_rgb=cv2.cvtColor(dst, cv2.COLOR_BGR2RGB)\n",
    "    print(pytesseract.image_to_string(dst_rgb, lang='Hangul+eng+kor'))\n",
    "\n",
    "cv2.imshow('src', src)\n",
    "cv2.imshow('dst_rgb', dst_rgb)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
